\chapter{Results}
\section{Evolution of Physical Quantities}
The simulation is run over a period of 250 years, in which one earthquake occurs on June 13th of the 195th year. This event can be clearly observed in \autoref{fig:timeEvolutionTANDEM_V} which depicts the maximum slip rate over time, which reaches $4.6m\cdot s^{-1}$ as opposed to an average of $1.0 \cdot 10^{-9}m\cdot s^{-1}$ in calm times. 
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
     	\centering
    	\includegraphics[width=1\textwidth]{images/TANDEMcompareFormulationstimeEvolutionVall.png}
       	\subcaption{Full simulation time} 
    \end{subfigure} 
    \begin{subfigure}{0.32\textwidth}
    	\centering
    	\includegraphics[width=1\textwidth]{images/TANDEMcompareFormulationstimeEvolutionVsurroundings.png}
       	\subcaption{Year of the earthquake} 
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
    	\centering
    	\includegraphics[width=1\textwidth]{images/TANDEMcompareFormulationstimeEvolutionVearthquake.png}
       	\subcaption{Evolution of the earthquake event} 
    \end{subfigure}
    \caption{Evolution of the maximal slip rate $V$ on the fault for different solvers on the symmetric two-dimensional BP1 problem with 200 elements on the fault}
    \label{fig:timeEvolutionTANDEM_V}
\end{figure}


\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
    	\centering
    	\includegraphics[width=1\textwidth]{images/TANDEMcompareFormulationstimeEvolutionDTall.png}
       	\subcaption{Full simulation time} 
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
    	\centering
    	\includegraphics[width=1\textwidth]{images/TANDEMcompareFormulationstimeEvolutionDTsurroundings.png}
       	\subcaption{Year of the earthquake} 
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
    	\centering
    	\includegraphics[width=1\textwidth]{images/TANDEMcompareFormulationstimeEvolutionDTearthquake.png}
       	\subcaption{Evolution of the earthquake event} 
    \end{subfigure}
    \caption{Evolution of the time step size $h$ for different solvers on the symmetric two-dimensional BP1 problem with 200 elements on the fault}
    \label{fig:timeEvolutionTANDEM_DT}
\end{figure}


\begin{figure}[H]
    \centering
    \begin{subfigure}{0.32\textwidth}
    	\centering
    	\includegraphics[width=1\textwidth]{images/TANDEMcompareFormulationstimeEvolutionRHSall.png}
       	\subcaption{Full simulation time} 
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
    	\centering
    	\includegraphics[width=1\textwidth]{images/TANDEMcompareFormulationstimeEvolutionRHSsurroundings.png}
       	\subcaption{Year of the earthquake} 
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
    	\centering
    	\includegraphics[width=1\textwidth]{images/TANDEMcompareFormulationstimeEvolutionRHSearthquake.png}
       	\subcaption{Evolution of the earthquake event} 
    \end{subfigure}
    \caption{Number of evaluations of the right hand side of the ODE in each time iteration for different solvers on the symmetric two-dimensional BP1 problem with 200 elements on the fault \newline
    \textbf{Legend:} \textcolor{red}{---} 1st order ODE formulation, \textcolor{blue}{---} 2nd order ODE formulation, \textcolor{green}{---} extended DAE formulation, \textcolor{yellow}{---} compact DAE formulation }
    \label{fig:timeEvolutionTANDEM_RHS}
\end{figure}


\section{Accuracy of the time integration}
The discussion in \autoref{ssec:LowerBoundTimeTolerance} only tackled the question of the lowest allowable tolerances to reach as accurate results as possible. However, such strict tolerances may severely reduce the timestep size and by consequence the length of the simulation. For many applications, satisfactory results are already obtained for larger tolerances, and the aim of this section is to investigate it for SEAS. As there is no mathematical definition for \textit{satisfactory results} (unless convergence is the only requirement), we will run the simulation with the minimum possible tolerance described in the previous section and assess how the results deteriorate as the tolerances increase. \\
We consider 


\section{Time adaptivity of BDF methods}
\subsection{Quality of the error estimates}
Two error estimates have been introduced for the BDF methods. First, in a given time step, the BDF scheme is applied a second time with higher order and the difference between both available solutions at the timestep yields the error estimate. The second method bases on the derivatives of Lagrangian polynomials as described in \autoref{sssec:errorEstimateBDFLagrange}, and has the advantage to be much cheaper to evaluate, since it does not require to solve the system again. It has already been shown in \autoref{ssec:QualityErrorEstimate_0D} that for the 0D example, the first method gives a more accurate estimation of the local truncation error than the second method. Both methods tend to overestimate the error, which is for sure a safe behavior, but restrict the choice of the next timesteps. The aim of this section is to see, whether similar estimates can be observed for the current problem. \\
Since here no exact solution is available for the slip or the state variable (or the slip rate, if the solution vector has an extended size), the accuracy of the error estimate cannot be directly determined as previously. Instead, we assume that the actual error is inferior to the estimate, and that the embedded method is more accurate than Lagrangian polynomials. We would then expect that the timestep adapter chooses larger timesteps with the first method and finishes the program earlier. Exactly this can be observed in \autoref{fig:BDFOrders_Lagrange_vs_Embedded_compact_ODE} for the compact ODE formulation. As expected, the embedded method is faster than Lagrangian polynomials, because features, such as an increase or decrease of the timestep size happen increasingly earlier with the embedded method. Overall, fewer timesteps are required to reach a solution at a given time. However, the allowed timesteps are only slightly larger and the difference cannot be noticed on the logarithmic scale. Nonetheless, it is positive to remark that the same small features such as intermediate peaks are observed with both methods, which indicates that both error estimates recognize the same inconsistencies in the solution. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{images/TANDEMTimeAnalysisDifferentBDFOrdersEmbedded_vs_Lagrange_CompactODE_Size5.png}
	\caption{Comparison of the chosen timestep sizes with the Lagrangian polynomials (full line ---) and the embedded error estimate (dashed line - -) for different BDF orders on a domain with 5 fault elements and the compact ODE formulation with $\varepsilon^S_a = \varepsilon^{\psi}_a = 10^{-7}$ and $\varepsilon^S_r = \varepsilon^{\psi}_r = 0$ }
	\label{fig:BDFOrders_Lagrange_vs_Embedded_compact_ODE}
\end{figure}
Overall, the embedded method gives a more accurate error estimate than the Lagrangian polynomials, and allows for larger timestep sizes. However, the gain is only very small, as in average 2\% fewer timesteps are required in sum at the end of the simulation, and does not justify the use of the embedded higher order BDF method as error estimate, as it is twice as expensive. Actually, the order of the BDF method has a much higher influence on the total number of iterations, as it can be seen in this graph and will be discussed more in detail in the next section. 


\subsection{Effect of the order on the timestep size}
The order of the BDF method has a strong influence on the total number of required timesteps, as it can be clearly seen in \autoref{fig:BDFOrders_Lagrange_vs_Embedded_compact_ODE} from the previous section. The simulation can be roughly split into four sections: the evolution of the aseismic slip, with an approximately constant stepsize of the order of $h=10^7$s, the evolution of the earthquake at $h=10^{-2}$s and the transition between both phases, where the stepsize progressively increases or decreases. In addition to that, there is the initialization phase, in which the stepsize starts at $h=0.1$s to quickly reach the stepsize of the aseismic slip. \\
From \autoref{fig:BDFOrders_Lagrange_vs_Embedded_compact_ODE}, it seems that the stepsize of the constant evolution directly depends on the BDF order: a lower order leads to a smaller timestep and thus requires much more steps in total until the end of the aseismic slip or earthquake. The transition from the aseismic slip to the earthquake is a crucial section, because the sudden increase of the slip rate requires to rapidly scale down the timestep size. The end of the earthquake is less critical, as the slip rate drops relatively slowly which gives more scope to the stepsize to adapt to it, moreover the eventual too small stepsize there is not as problematic as a too high stepsize at the beginning of the earthquake. It is also interesting to look at the initialization phase, as there are no other constraints for a small stepsize but the limitations of the BDF scheme. \\
\autoref{fig:BDFOrders_compact_ODE_compare_initialization} shows exactly the increase at the beginning of the simulation for the BDF methods of order two to six and \autoref{fig:BDFOrders_compact_ODE_compare_begin_first_EQ} shows the transition from the aseismic slip to the first earthquake. To better contextualize the results, the graph for the explicit 4th order Runge-Kutta scheme with the same tolerances is also shown. In the initialization phase, the slope is in general steeper if a higher order method is used. One notable exception is the 6th order BDF scheme, which performs almost as bad as the 2nd order scheme. Unlike the other schemes, it does not increase smoothly but has regular steps, which indicates that the 6th order scheme is not as stable as the others. Overall, the increase is perfectly exponential for the schemes 2-5, and the factor by which the timestep size increases at each step is driven by the order of the method. Interestingly enough, in the very first timesteps,  the timestep size of the 2nd order scheme jumps by two orders of magnitude whereas the 5th and 6th order do not benefit at all from such a strong initial increase. At the transition from the aseismic slip to the earthquake phase, the timestep size decreases exponentially with the same pattern: a higher order scheme allows a stronger reduction of the timestep size from one step to the next. Now, the 6th order scheme is perfectly stable and has the steepest slope among all BDF methods. When compared to the explicit 4th order Runge-Kutta method, all BDF methods in both scenarios perform extremely bad. This is however not a reason to discard BDF methods as a whole, since they are much more flexible with respect to the defined tolerances, as it will be discussed in a later stage.

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{images/TANDEMTimeAnalysisDifferentBDFOrdersLagrange_CompactODE_Size5_Init.png}
		\subcaption{Maximum absolute value of the friction law} 
		\label{fig:BDFOrders_compact_ODE_compare_initialization}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{images/TANDEMTimeAnalysisDifferentBDFOrdersLagrange_CompactODE_Size5_Begin1stEQ.png}
		\subcaption{Absolute change of the friciton law per timestep, averaged over 50 timesteps} 
		\label{fig:BDFOrders_compact_ODE_compare_begin_first_EQ}
	\end{subfigure}
	\caption{Evolution of the time step size for BDF schemes with different order at key points in the simulation on a domain with 5 fault elements and the compact ODE formulation with $\varepsilon^S_a = \varepsilon^{\psi}_a = 10^{-7}$ and $\varepsilon^S_r = \varepsilon^{\psi}_r = 0$ }
\end{figure}
Higher order BDF methods allow in general for higher stepsizes and are to be preferred over lower order methods. Since the costs of all BDF schemes are similar (they converge similarly fast and each Newton step always requires to solve one linear system), there is no strong reason to use a low order scheme here. Because of the stability issues with the 6th order scheme when the stepsize increases, the best performance can be expected from the 5th order scheme. There are some few cases where lower order methods perform better, such at the very beginning of the simulation. To take advantage of these peculiarities, and also to use the 6th order method if it is currently stable, the order of the BDF scheme can be adapted at each timestep as described in \autoref{sssec:adaptiveBDFOrder}. In this case, each step also calculates by how much the timestep size would change if one order higher or lower was used, and if a larger timestep can be reached, the order is changed for the next timestep. This has been implemented and the performance of the order-adaptive BDF scheme is shown in \autoref{fig:BDFOrders_compact_ODE_compare_adaptiveBDF}. It successfully uses the scheme with the largest allowable stepsize and finishes the simulation faster than any other methods. 


\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{images/TANDEMTimeAnalysisDifferentBDFOrders_Lagrange_CompactODE_Size5_FULLTIME.png}
	\caption{Evolution of the time step size for the adaptive-order BDF method and for a selection of different fixed orders on a domain with 5 fault elements and the compact ODE formulation with $\varepsilon^S_a = \varepsilon^{\psi}_a = 10^{-7}$ and $\varepsilon^S_r = \varepsilon^{\psi}_r = 0$ }
	\label{fig:BDFOrders_compact_ODE_compare_adaptiveBDF}
\end{figure}


\section{Error estimation of the 2nd order ODE formulation}
Despite the lack of an analytic solution to the problem, this formulation of the problem offers a very powerful tool to evaluate the accuracy of the results. Since the slip rate is not calculated from the friction law anymore, it is not ensured that it is always equal to 0 up to numerical precision, as it was the case in the first order formulations of the SEAS problem. We could then evaluate the friction law at every timestep to assess the accuracy of the numerical integration. A large deviation from 0 of the absolute value of the friction law at a given timestep means that the integrator provides poor results. For a standard execution of the simulation with the second order formulation, this metric is not available, since the evaluation of $\tau$ in the friction law requires to solve the Poisson problem, and the big advantage of the new formulation is exactly not to solve this system. For the following graphs, The value of the friction law has been exceptionally calculated. \\

For the first set of pictures, a very small domain with 5 fault elements has been chosen to be able to observe the evolution of the quantities over a long period of time. The tolerances for the slip and the state variable have been fixed to $10^{-7}$. \autoref{fig:timeEvolution_2ndOrderODE_differentTolerances} shows the maximum value of the friction law for varying relative relative tolerances for $V$, and the absolute tolerance is fixed to 0. From the initial condition, the slip rate is calculated to fulfill the friction law up to numerical precision, at about $10^{-15}$. After some time steps, this high precision is lost, and at every earthquake event, the difference increases sharply. Overall, the logarithmic shape of the curves indicate a linear decrease of accuracy with time, It seems that, at every evaluation of the right-hand side function, for one given tolerance of $V$, a certain local truncation error is added to the residual of the friction law. At an earthquake, much more timesteps are required and the sum of the local truncation errors sum up to form an apparent sharp increase in the global error. For lower tolerances in $V$, the increase of the error at every evaluation is smaller the accuracy is higher. This hypothesis is confirmed by \autoref{fig:timeEvolution_LTE_2ndOrderODE_differentTolerances}, which shows the absolute change of the friction law per timestep. It can be considered somehow as an estimate for the local truncation error (LTE), as it describes how the error increases at each step. For a given tolerance in the slip rate, an upper bound for the LTE can be observed. During the earthquake, the LTE is actually much lower than in the aseismic slip. Only the high number of steps required for this event result in the apparent sharp increase of the total error. 

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{images/TANDEMtimeEvolutionFExtendedODEDifferentTolerances.png}
		\subcaption{Maximum absolute value of the friction law} 
		\label{fig:timeEvolution_2ndOrderODE_differentTolerances}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{images/TANDEMtimeEvolutionFLTEExtendedODEDifferentTolerances.png}
		\subcaption{Absolute change of the friciton law per timestep, averaged over 50 timesteps} 
		\label{fig:timeEvolution_LTE_2ndOrderODE_differentTolerances}
	\end{subfigure}
	\caption{Evolution of the value of the friction law of the 2nd order formulation with 5 elements on the fault over 1000 years with varying relative tolerances in the slip rate $V$}
\end{figure}

The local truncation error seems to depend linearly on the tolerance for the slip rate, and a similar dependency is to be expected with respect to the tolerances of the slip $S$ and $\psi$, since these quantities are also required to evaluate the friction law. Next, we investigate whether other parameters, such as the spatial discretization, have an effect on the global error too. For a fixed relative tolerance for the slip of $10^{-7}$, the simulation has been executed for different spatial resolutions and the evolution of the friction law in each case is shown in \autoref{fig:timeEvolution_2ndOrderODE_differentSizes}. Since each fault elements contains three fault nodes, one has to multiply $n$ by three to obtain the total number of simulated fault nodes. At the beginning, all domain sizes have a similar accuracy, but after some earthquakes, it seems that the error in domains with higher resolution increases less fast. However, in \autoref{fig:timeEvolution_LTE_2ndOrderODE_differentSizes}, it is hard to distinguish separate upper bounds for the local truncation error with different domain sizes. The better performance of larger domains is rather due to the overall higher accuracy of simulations with higher resolutions and to a lower incidence of earthquakes, than due to a reduction of the LTE. 

\begin{figure}[H]
	\centering
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{images/TANDEMtimeEvolutionFExtendedODEDifferentSizes.png}
		\subcaption{Maximum absolute value of the friction law} 
		\label{fig:timeEvolution_2ndOrderODE_differentSizes}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\centering
		\includegraphics[width=1\textwidth]{images/TANDEMtimeEvolutionFLTEExtendedODEDifferentSizes.png}
		\subcaption{Absolute change of the friciton law per timestep, averaged over 50 timesteps} 
		\label{fig:timeEvolution_LTE_2ndOrderODE_differentSizes}
	\end{subfigure}
	\caption{Evolution of the value of the friction law of the 2nd order formulation over 400 years with varying domain sizes, where $n$ denotes the number of fault elements}
\end{figure}

Overall, the evaluation of the friction law is a suitable estimate for the global error in the system. Over the course of time, its value increases step wise at each earthquake, but overall linearly, which is due to a regular local truncation error. The main driver of the LTE is the tolerance for the slip rate $V$, which has been introduced to the system along with the second order ODE formulation. The upper bound for the LTE is proportional to the chosen tolerance.

\section{Scalability}

\section{Runtime Analysis}
